{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div align=\"right\">\n",
        "    <img src=\"https://pnsqc.org/photos/Screen_Shot_2023-12-20_at_2.38.38_PM_02162024162545.PNG\">\n",
        "</div>"
      ],
      "metadata": {
        "id": "thQhRY7NsgaC"
      },
      "id": "thQhRY7NsgaC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective"
      ],
      "metadata": {
        "id": "OnMY8Oppswb1"
      },
      "id": "OnMY8Oppswb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this notebook is to generate software performance testing solution using Gemini - Google's Generative AI Model. Gemini supports multimodals, combining different types of data like text, code, audio, image and video."
      ],
      "metadata": {
        "id": "GuRsHau6t9-3"
      },
      "id": "GuRsHau6t9-3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "x09E_hBpuM15"
      },
      "id": "x09E_hBpuM15"
    },
    {
      "cell_type": "markdown",
      "id": "16cfeb36",
      "metadata": {
        "id": "16cfeb36"
      },
      "source": [
        "## Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f166bbf",
      "metadata": {
        "id": "1f166bbf"
      },
      "outputs": [],
      "source": [
        "# install gen ai library\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fiS81FyunZFJ",
      "metadata": {
        "id": "fiS81FyunZFJ"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yPNtJebH7AOC",
      "metadata": {
        "id": "yPNtJebH7AOC"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import pathlib\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQhsiDnIPtfB",
        "outputId": "cfcfa399-b59e-4e46-eefe-3a9be30af028"
      },
      "id": "IQhsiDnIPtfB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jrr_Go-onnjE",
      "metadata": {
        "id": "Jrr_Go-onnjE"
      },
      "source": [
        "## Setup API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8LzUywwG7J_s",
      "metadata": {
        "id": "8LzUywwG7J_s"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mNWsUrtS6520",
      "metadata": {
        "id": "mNWsUrtS6520"
      },
      "outputs": [],
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection"
      ],
      "metadata": {
        "id": "uArCO--3u7Mw"
      },
      "id": "uArCO--3u7Mw"
    },
    {
      "cell_type": "markdown",
      "id": "Zk6r6_ELnTjA",
      "metadata": {
        "id": "Zk6r6_ELnTjA"
      },
      "source": [
        "## List Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E8dqFY_p5wsm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "E8dqFY_p5wsm",
        "outputId": "1c67b4b7-c758-4372-e6ab-04a94f5fd056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "# List Google AI Gemini Models\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mmIQGJECnu4o",
      "metadata": {
        "id": "mmIQGJECnu4o"
      },
      "source": [
        "## Model Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lfoawnxv6abs",
      "metadata": {
        "id": "Lfoawnxv6abs"
      },
      "outputs": [],
      "source": [
        "# Generation config\n",
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.5,\n",
        "  \"top_k\": 0,\n",
        "  \"max_output_tokens\": 7000,\n",
        "}\n",
        "\n",
        "# Safety config\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation"
      ],
      "metadata": {
        "id": "_wFX23HyX0Uq"
      },
      "id": "_wFX23HyX0Uq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model with the selected model name and configs\n",
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "XGGeh9yyUq9z"
      },
      "id": "XGGeh9yyUq9z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases"
      ],
      "metadata": {
        "id": "FXdgbxdluULb"
      },
      "id": "FXdgbxdluULb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selected \"gemini-1.0-pro\" Google AI model, is used to enhance Software Performance Testing. The main idea behind the 'Gen AI Performance Tester' app is to reduce the overall testing effort. This is possible by automating time-consuming activities involved in each testing phase.\n",
        "\n",
        "Following five major processes in Software Testing Life Cycle [STLC] were identified as use cases."
      ],
      "metadata": {
        "id": "Tf-15V7gutjt"
      },
      "id": "Tf-15V7gutjt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Test Plan Creation"
      ],
      "metadata": {
        "id": "uBaUIMs2r4Yj"
      },
      "id": "uBaUIMs2r4Yj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j5lAx9Ft53ss",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "j5lAx9Ft53ss",
        "outputId": "b835e8ec-3374-40d3-f0cc-336effd3903d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.5 ms, sys: 11.7 ms, total: 60.1 ms\n",
            "Wall time: 8.06 s\n"
          ]
        }
      ],
      "source": [
        "# Generate content for the prompt and display the execution time\n",
        "%%time\n",
        "response1 = model.generate_content(\"Create a performance test plan for API load testing in Cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E4JFAtX759tB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E4JFAtX759tB",
        "outputId": "a0eea4a9-4f0a-4f9e-a7bd-53647f417ead"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Performance Test Plan for API Load Testing in Cloud**\n> \n> **Objective:**\n> \n> * Evaluate the performance and scalability of an API under various load conditions.\n> \n> **Scope:**\n> \n> * This test plan covers load testing of the following API: [API Name]\n> * The test will simulate realistic user traffic patterns.\n> \n> **Test Environment:**\n> \n> * Cloud platform: [Cloud Platform Name]\n> * Region: [Region]\n> * API endpoint: [API Endpoint URL]\n> * Load generator: [Load Generator Tool]\n> \n> **Test Scenarios:**\n> \n> * **Scenario 1: Baseline Load**\n>     * Number of concurrent users: 100\n>     * Request rate: 10 requests per second\n>     * Duration: 30 minutes\n> * **Scenario 2: Peak Load**\n>     * Number of concurrent users: 1000\n>     * Request rate: 50 requests per second\n>     * Duration: 30 minutes\n> * **Scenario 3: Stress Load**\n>     * Number of concurrent users: 2000\n>     * Request rate: 100 requests per second\n>     * Duration: 15 minutes\n> \n> **Test Metrics:**\n> \n> * **Response time (ms)**\n> * **Throughput (requests per second)**\n> * **Error rate (%)**\n> * **Resource utilization (CPU, memory)**\n> \n> **Test Procedure:**\n> \n> 1. Configure the load generator with the test scenarios.\n> 2. Start the load generator and monitor the test metrics.\n> 3. Adjust the load parameters as needed to achieve the desired load conditions.\n> 4. Collect and analyze the test results.\n> \n> **Acceptance Criteria:**\n> \n> * The API should handle the peak load scenario without significant performance degradation.\n> * The error rate should be below 1% under all load conditions.\n> * The resource utilization should not exceed 80% during the peak load scenario.\n> \n> **Reporting:**\n> \n> * A test report will be generated that includes the following:\n>     * Test scenarios\n>     * Test results\n>     * Analysis and recommendations\n> \n> **Timeline:**\n> \n> * Test preparation: 1 week\n> * Test execution: 1 day\n> * Test analysis and reporting: 1 week\n> \n> **Responsibilities:**\n> \n> * Test lead: [Test Lead Name]\n> * Load generator operator: [Load Generator Operator Name]\n> * API developer: [API Developer Name]"
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Print the response - test plan document\n",
        "to_markdown(response1.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the prompt safety ratings\n",
        "response1.candidates[0].safety_ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijy88_Z8FxWO",
        "outputId": "b67ef492-c458-417f-a7b6-3222d66adb34"
      },
      "id": "ijy88_Z8FxWO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "probability: NEGLIGIBLE\n",
              ", category: HARM_CATEGORY_HATE_SPEECH\n",
              "probability: NEGLIGIBLE\n",
              ", category: HARM_CATEGORY_HARASSMENT\n",
              "probability: NEGLIGIBLE\n",
              ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "probability: NEGLIGIBLE\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Test Data Generation"
      ],
      "metadata": {
        "id": "Z6Ehurg9r_HE"
      },
      "id": "Z6Ehurg9r_HE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ipZ24dxZqfet",
      "metadata": {
        "id": "ipZ24dxZqfet",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c277c53e-c037-4216-ba24-0c7806ab74f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 33 ms, sys: 4.93 ms, total: 37.9 ms\n",
            "Wall time: 6.06 s\n"
          ]
        }
      ],
      "source": [
        "# Generate content for the prompt and display the execution time\n",
        "%%time\n",
        "response2 = model.generate_content(\"Generate 50 test data of name,age, and city for load testing and display in csv format\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KLcGJ5aDqrDp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "KLcGJ5aDqrDp",
        "outputId": "b11b8b01-85d0-4fca-bad7-6997536eb2e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **CSV Format:**\n> \n> ```csv\n> name,age,city\n> ```\n> \n> **Test Data:**\n> \n> ```csv\n> John,25,New York\n> Jane,30,Los Angeles\n> Michael,28,Chicago\n> Sarah,22,San Francisco\n> David,35,Dallas\n> Emily,26,Seattle\n> Matthew,32,Houston\n> Jessica,24,Phoenix\n> Mark,31,Philadelphia\n> Ashley,27,San Diego\n> Brian,33,Boston\n> Lauren,23,Miami\n> James,36,Atlanta\n> Jennifer,29,Denver\n> Robert,34,Detroit\n> Elizabeth,21,Baltimore\n> William,37,Minneapolis\n> Michelle,25,Cleveland\n> Thomas,30,Columbus\n> Amanda,28,Indianapolis\n> Charles,35,Nashville\n> Maria,26,Memphis\n> Christopher,32,Charlotte\n> Victoria,24,Raleigh\n> Richard,31,Oklahoma City\n> Abigail,27,Portland\n> Joseph,33,Las Vegas\n> Nicole,23,Milwaukee\n> Daniel,36,Kansas City\n> Anna,29,Jacksonville\n> Patrick,34,Austin\n> Katherine,21,San Antonio\n> George,37,New Orleans\n> Sophia,25,Louisville\n> Anthony,30,Birmingham\n> Olivia,28,Memphis\n> Ethan,35,Nashville\n> Isabella,26,Charlotte\n> Benjamin,32,Raleigh\n> Ava,24,Oklahoma City\n> Alexander,31,Portland\n> Mia,27,Las Vegas\n> Elijah,33,Milwaukee\n> Madison,23,Kansas City\n> ```"
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Print the response - csv dataset\n",
        "to_markdown(response2.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the prompt safety ratings\n",
        "response2.candidates[0].safety_ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpNQ6TA9BPoG",
        "outputId": "7f70ced7-05bb-44fc-9462-ff5e02db6051"
      },
      "id": "BpNQ6TA9BPoG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "probability: NEGLIGIBLE\n",
              ", category: HARM_CATEGORY_HATE_SPEECH\n",
              "probability: NEGLIGIBLE\n",
              ", category: HARM_CATEGORY_HARASSMENT\n",
              "probability: NEGLIGIBLE\n",
              ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "probability: NEGLIGIBLE\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Test Script Generation"
      ],
      "metadata": {
        "id": "9dyV-YaWKshw"
      },
      "id": "9dyV-YaWKshw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate content for the prompt and display the execution time\n",
        "%%time\n",
        "response3 = model.generate_content(\"Generate Gatling code to performance test a REST API with 50 users for 30 minutes and 10 seconds think time\")"
      ],
      "metadata": {
        "id": "wO9_4_1_KyJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7773a0c5-736c-4c6f-87b2-72aab94f7cb9"
      },
      "id": "wO9_4_1_KyJ7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.9 ms, sys: 3.45 ms, total: 25.4 ms\n",
            "Wall time: 3.73 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response - Gatling script\n",
        "to_markdown(response3.text)"
      ],
      "metadata": {
        "id": "xk6XkCywLAMS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "604f8c45-9b06-49e2-a03f-4a363703d6e8"
      },
      "id": "xk6XkCywLAMS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ```gatling\n> import io.gatling.core.Predef._\n> import io.gatling.http.Predef._\n> \n> class RestApiSimulation extends Simulation {\n> \n>   val httpProtocol = http\n>     .baseUrl(\"http://localhost:8080\")\n>     .acceptHeader(\"application/json\")\n> \n>   val scn = scenario(\"Rest API Test\")\n>     .exec(\n>       http(\"Get all users\")\n>         .get(\"/users\")\n>         .check(status.is(200))\n>     )\n>     .pause(10)\n> \n>   setUp(\n>     scn.inject(\n>       rampUsers(50) over (10 seconds),\n>       constantUsersPerSec(50) during (30 minutes)\n>     ).protocols(httpProtocol)\n>   )\n> }\n> ```"
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Test Docker Container Creation"
      ],
      "metadata": {
        "id": "k3jNA6MiQ5lm"
      },
      "id": "k3jNA6MiQ5lm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate content for the prompt and display the execution time\n",
        "%%time\n",
        "response4 = model.generate_content(\"Generate a Docker script to create a server with Linux operating system, Java 17, and Gatling tool installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Zck9YgFlRTDg",
        "outputId": "8007a3a8-a8a3-4070-a96a-a2133fe11382"
      },
      "id": "Zck9YgFlRTDg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.3 ms, sys: 5.59 ms, total: 27.9 ms\n",
            "Wall time: 4.43 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response - Docker script\n",
        "to_markdown(response4.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "qaVajWgsRXPq",
        "outputId": "6dee465f-e366-4495-c123-06d887869f33"
      },
      "id": "qaVajWgsRXPq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ```dockerfile\n> FROM ubuntu:20.04\n> \n> # Update the package list\n> RUN apt-get update\n> \n> # Install Java 17\n> RUN apt-get install -y openjdk-17-jdk\n> \n> # Install Gatling\n> RUN wget https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts/3.7.4/gatling-charts-highcharts-3.7.4.zip\n> RUN unzip gatling-charts-highcharts-3.7.4.zip\n> RUN mv gatling-charts-highcharts-3.7.4 /opt/gatling\n> \n> # Create a user for running Gatling\n> RUN useradd -m -s /bin/bash gatling\n> \n> # Set the environment variables for Gatling\n> RUN echo \"export GATLING_HOME=/opt/gatling\" >> /etc/profile\n> RUN echo \"export PATH=$GATLING_HOME/bin:$PATH\" >> /etc/profile\n> \n> # Start Gatling\n> CMD [\"/bin/bash\"]\n> ```"
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Test Report Generation"
      ],
      "metadata": {
        "id": "0UlMo09GMFj7"
      },
      "id": "0UlMo09GMFj7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read input file\n",
        "url = \"Test_Results.csv\"\n",
        "with open(url) as file:\n",
        "  input_file = file.read()\n",
        "\n",
        "# Print first 5 lines\n",
        "for line in input_file.splitlines()[:5]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MedQ8vU7X55g",
        "outputId": "a2e68c5c-7c93-4e41-8c57-5a59cd641178"
      },
      "id": "MedQ8vU7X55g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timeStamp,elapsed,label,responseCode,responseMessage,threadName,dataType,success,failureMessage,bytes,sentBytes,grpThreads,allThreads,URL,Latency,IdleTime,Connect\n",
            "1652128008147,16,Test_01_Search_FirstName,200,HTTP/1.1 200,Machine_Learning_APIs 1-1,text,true,,289,0,1,1,http://localhost:8080/search/fname?firstName=Mary,15,0,0\n",
            "1652128008240,9,Test_03_Search_Email,200,HTTP/1.1 200,Machine_Learning_APIs 1-2,text,true,,288,0,2,2,http://localhost:8080/search/email?email=alexwill%40gmail.com,8,0,0\n",
            "1652128008348,4,Test_05_Search_AccountId,200,HTTP/1.1 200,Machine_Learning_APIs 1-3,text,true,,289,0,3,3,http://localhost:8080/search/accountid?accountId=1634589017,4,0,0\n",
            "1652128008458,3,Test_01_Search_FirstName,200,HTTP/1.1 200,Machine_Learning_APIs 1-4,text,true,,289,0,4,4,http://localhost:8080/search/fname?firstName=Mary,3,0,0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate content for the prompt and display the execution time\n",
        "%%time\n",
        "response5 = model.generate_content(['Create a performance test summary report with findings and recommendations for the test results in ', input_file])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MOyVRCPIYVIb",
        "outputId": "91a3235d-c314-45ef-dfdb-f928a39f8208"
      },
      "id": "MOyVRCPIYVIb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 60.3 ms, sys: 2.08 ms, total: 62.4 ms\n",
            "Wall time: 11.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response - test summary report document\n",
        "print(response5.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yXkbeYoDsrR",
        "outputId": "7d4db629-15e2-4f8d-ec84-3eaf93cc5207"
      },
      "id": "2yXkbeYoDsrR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Performance Test Summary Report**\n",
            "\n",
            "**Test Results:**\n",
            "\n",
            "* **Total Tests:** 100\n",
            "* **Successful Tests:** 100\n",
            "* **Failed Tests:** 0\n",
            "\n",
            "**Response Time Metrics:**\n",
            "\n",
            "* **Average Response Time:** 15 ms\n",
            "* **Median Response Time:** 14 ms\n",
            "* **90th Percentile Response Time:** 18 ms\n",
            "* **95th Percentile Response Time:** 20 ms\n",
            "* **99th Percentile Response Time:** 25 ms\n",
            "\n",
            "**Resource Utilization Metrics:**\n",
            "\n",
            "* **Average CPU Utilization:** 5%\n",
            "* **Average Memory Utilization:** 10%\n",
            "\n",
            "**Findings:**\n",
            "\n",
            "* The API performed consistently well under load, with all requests completing successfully.\n",
            "* Response times were within acceptable limits, with the majority of requests completing in under 20 ms.\n",
            "* Resource utilization was low, indicating that the API can handle a higher load without performance degradation.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "* Consider increasing the number of worker threads to further improve response times.\n",
            "* Monitor resource utilization closely to ensure that the API remains performant under peak load.\n",
            "* Implement caching mechanisms to reduce the load on the database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Responsible AI"
      ],
      "metadata": {
        "id": "v9sglZIgh6IG"
      },
      "id": "v9sglZIgh6IG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the potential of AI is immense, these technologies can also raise critical challenges that need to be addressed thoughtfully, and carefully. Misuse of AI technologies can result in unintended or unforeseen consequences. Hence it is important to develop technology responsibly. Following are some of the Responsible AI objectives incorporated in 'Gen AI Performance Tester' app.\n",
        "- **Be socially beneficial**: The app is intended to be useful for software testing professionals in terms of time and effort. This in turn helps to reduce the overall project budget as well.\n",
        "- **Be built and tested for safety**: Safety is a major concern of Generative AI applications using prompt engineering. In this app, safety is ensured by blocking harmful contents of level medium and above using the parameter, 'BLOCK_MEDIUM_AND_ABOVE'.\n",
        "- **Be accountable to people**: The Gen AI Performance Tester app is intended to be an aid for software performance testers. Testers using the app have a key role in further app improvement by providing feedback.\n"
      ],
      "metadata": {
        "id": "YGKN_94QiA8K"
      },
      "id": "YGKN_94QiA8K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impact"
      ],
      "metadata": {
        "id": "ogDdGFAPd9IX"
      },
      "id": "ogDdGFAPd9IX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The app can positively affect the software performance testing community in many ways. Some of the details are discussed below:\n",
        "- **Benefits**: The major benefit of the app is that it acts as a single source of access, spanning across the Software Performance Testing Life Cycle.It automates various performance testing activities, thereby reducing the efforts.\n",
        "- **Ease of us**e: This app is easy to use and can also be customised for project specific scenarios by modifying the prompt accordingly.\n",
        "- **Real world application**: The Gen AI Performance Tester solution is envisioned to be an aid for software performance testers who struggle with project budget and timeline constraints. The app can be made available on any cloud platform or can be deployed as an on-premise solution.\n"
      ],
      "metadata": {
        "id": "-vAV2keAeJYo"
      },
      "id": "-vAV2keAeJYo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Road Ahead"
      ],
      "metadata": {
        "id": "Bk3rYonnda5E"
      },
      "id": "Bk3rYonnda5E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Gen AI Performance Tester' solution was built using Google's AI model, Gemini. From among various available options, \"gemini-1.0-pro\" model was used. In future, the app can be enhanced to include more performance testing and performance engineering capabilities. Some of the major use cases are given below:\n",
        "- Detect anomalies in transaction response time or server utilization\n",
        "- Root cause analysis of performance issues\n",
        "- Forecast application behavior based on previous events or logs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "id3M3_zqdh8B"
      },
      "id": "id3M3_zqdh8B"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fiS81FyunZFJ",
        "Jrr_Go-onnjE",
        "uArCO--3u7Mw",
        "Zk6r6_ELnTjA",
        "mmIQGJECnu4o",
        "_wFX23HyX0Uq",
        "uBaUIMs2r4Yj",
        "Z6Ehurg9r_HE",
        "9dyV-YaWKshw",
        "k3jNA6MiQ5lm",
        "0UlMo09GMFj7",
        "ogDdGFAPd9IX",
        "Bk3rYonnda5E",
        "qjLVjvLjdGQ2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}